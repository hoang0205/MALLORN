{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14010596,"sourceType":"datasetVersion","datasetId":8925232}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport lightgbm as lgb\nfrom scipy.optimize import curve_fit\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom joblib import Parallel, delayed\nimport warnings\nimport torch\nfrom tqdm.auto import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T10:35:43.229703Z","iopub.execute_input":"2025-12-24T10:35:43.229914Z","iopub.status.idle":"2025-12-24T10:35:47.813485Z","shell.execute_reply.started":"2025-12-24T10:35:43.229893Z","shell.execute_reply":"2025-12-24T10:35:47.812740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T10:35:58.284033Z","iopub.execute_input":"2025-12-24T10:35:58.284712Z","iopub.status.idle":"2025-12-24T10:35:58.288263Z","shell.execute_reply.started":"2025-12-24T10:35:58.284685Z","shell.execute_reply":"2025-12-24T10:35:58.287521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\n\nclass DevNull:\n    def write(self, msg): pass\n    def flush(self): pass\n\nsys.stderr = DevNull()\nUSE_GPU = torch.cuda.is_available()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T10:36:03.617328Z","iopub.execute_input":"2025-12-24T10:36:03.617924Z","iopub.status.idle":"2025-12-24T10:36:03.621550Z","shell.execute_reply.started":"2025-12-24T10:36:03.617896Z","shell.execute_reply":"2025-12-24T10:36:03.621006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_PATH = '/kaggle/input/mallorn-dataset' \nprint(f\"Data Path: {DATA_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T10:36:06.452115Z","iopub.execute_input":"2025-12-24T10:36:06.452811Z","iopub.status.idle":"2025-12-24T10:36:06.456357Z","shell.execute_reply.started":"2025-12-24T10:36:06.452782Z","shell.execute_reply":"2025-12-24T10:36:06.455733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bazin_func(t, A, B, t0, tau_fall, tau_rise):\n    with np.errstate(over='ignore', invalid='ignore'):\n        flux = A * (np.exp(-(t - t0) / tau_fall) / (1 + np.exp(-(t - t0) / tau_rise))) + B\n    return np.nan_to_num(flux)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T10:36:07.992862Z","iopub.execute_input":"2025-12-24T10:36:07.993663Z","iopub.status.idle":"2025-12-24T10:36:07.997452Z","shell.execute_reply.started":"2025-12-24T10:36:07.993626Z","shell.execute_reply":"2025-12-24T10:36:07.996799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fit_bazin(time, flux, flux_err):\n    if len(time) < 5: \n        return {k: np.nan for k in ['A', 'B', 't0', 'tau_fall', 'tau_rise', 'chi2']}\n\n    peak_idx = np.argmax(flux)\n    # Initial guesses\n    p0 = [flux[peak_idx], np.min(flux), time[peak_idx], 50.0, 10.0]\n    # Bounds\n    bounds = ([0, -np.inf, time.min()-50, 1e-3, 1e-3], [np.inf, np.inf, time.max()+50, 500, 500])\n\n    try:\n        popt, _ = curve_fit(bazin_func, time, flux, p0=p0, sigma=flux_err, bounds=bounds, maxfev=1000)\n        residuals = flux - bazin_func(time, *popt)\n        chi2 = np.sum((residuals / flux_err)**2) / (len(time) - 5)\n        return {'A': popt[0], 'B': popt[1], 't0': popt[2], 'tau_fall': popt[3], 'tau_rise': popt[4], 'chi2': chi2}\n    except:\n        return {k: np.nan for k in ['A', 'B', 't0', 'tau_fall', 'tau_rise', 'chi2']}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T10:36:09.540740Z","iopub.execute_input":"2025-12-24T10:36:09.541469Z","iopub.status.idle":"2025-12-24T10:36:09.547176Z","shell.execute_reply.started":"2025-12-24T10:36:09.541444Z","shell.execute_reply":"2025-12-24T10:36:09.546665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_gp_prediction(time, flux, flux_err, t_query):\n    if len(time) < 3: return np.nan\n    kernel = C(1.0) * RBF(length_scale=20.0) + WhiteKernel(noise_level=1.0)\n    gp = GaussianProcessRegressor(kernel=kernel, alpha=flux_err**2, n_restarts_optimizer=0)\n    try:\n        gp.fit(time.reshape(-1, 1), flux)\n        pred, _ = gp.predict(np.array([[t_query]]), return_std=True)\n        return pred[0]\n    except:\n        return np.nan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T10:36:11.892012Z","iopub.execute_input":"2025-12-24T10:36:11.892316Z","iopub.status.idle":"2025-12-24T10:36:11.896915Z","shell.execute_reply.started":"2025-12-24T10:36:11.892290Z","shell.execute_reply":"2025-12-24T10:36:11.896345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_stetson(flux, flux_err):\n    n = len(flux)\n    if n < 2: return np.nan, np.nan\n    \n    mean_flux = np.mean(flux)\n    delta = (flux - mean_flux) / (flux_err + 1e-6) \n    \n    abs_delta_mean = np.mean(np.abs(delta))\n    delta_sq_mean = np.mean(delta**2)\n    k = (1 / np.sqrt(n)) * (abs_delta_mean / np.sqrt(delta_sq_mean))\n\n    j = np.sum(np.sign(delta[:-1] * delta[1:]) * np.sqrt(np.abs(delta[:-1] * delta[1:])))\n    j = (j / (n - 1)) * np.sign(j) \n    \n    return j, k","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T10:36:14.109874Z","iopub.execute_input":"2025-12-24T10:36:14.110172Z","iopub.status.idle":"2025-12-24T10:36:14.114912Z","shell.execute_reply.started":"2025-12-24T10:36:14.110150Z","shell.execute_reply":"2025-12-24T10:36:14.114315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_chi2_red(flux, flux_err):\n    if len(flux) < 2: return np.nan\n    w_mean = np.average(flux, weights=1/(flux_err**2 + 1e-9))\n    chi2 = np.sum(((flux - w_mean)**2) / (flux_err**2 + 1e-9))\n    return chi2 / (len(flux) - 1)\n\ndef calculate_flux_excess(flux, flux_err):\n    if len(flux) < 2: return np.nan\n    return (np.max(flux) - np.median(flux)) / (np.std(flux) + 1e-9)\n\ndef calculate_abs_mag(peak_flux, z):\n    if np.isnan(z) or z <= 0 or peak_flux <= 0:\n        return np.nan\n    try:\n        d_L = cosmo.luminosity_distance(z).value * 1e6 # đổi sang parsec\n        m_app = -2.5 * np.log10(peak_flux)\n        M_abs = m_app - 5 * (np.log10(d_L) - 1)\n        return M_abs\n    except:\n        return np.nan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:10:54.072709Z","iopub.execute_input":"2025-12-24T11:10:54.073379Z","iopub.status.idle":"2025-12-24T11:10:54.080331Z","shell.execute_reply.started":"2025-12-24T11:10:54.073349Z","shell.execute_reply":"2025-12-24T11:10:54.079646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_single_object(obj_id, df_obj, redshift):\n    feats = {'object_id': obj_id}\n    \n    # Chuẩn hóa thời gian\n    t_min = df_obj['Time (MJD)'].min()\n    df_obj['Time_Rel'] = df_obj['Time (MJD)'] - t_min\n    \n    filters = ['u', 'g', 'r', 'i', 'z', 'y']\n    peak_time = np.nan\n    max_flux_global = -np.inf\n    \n    filter_data = {} \n    \n    for f in filters:\n        df_f = df_obj[df_obj['Filter'] == f]\n        if df_f.empty:\n            for stat in ['chi2_red', 'excess', 'bazin_rise_fall_ratio']:\n                feats[f'{f}_{stat}'] = np.nan\n            continue\n            \n        flux = df_f['Flux'].values\n        flux_err = df_f['Flux_err'].values\n        time = df_f['Time_Rel'].values\n        \n        feats[f'{f}_chi2_red'] = calculate_chi2_red(flux, flux_err)\n        feats[f'{f}_excess'] = calculate_flux_excess(flux, flux_err)\n        \n        feats[f'{f}_mean'] = np.mean(flux)\n        feats[f'{f}_min'] = np.min(flux)\n        feats[f'{f}_skew'] = pd.Series(flux).skew() # Cần pandas để tính skew nhanh\n        \n        j, k = calculate_stetson(flux, flux_err)\n        feats[f'{f}_stetson_j'] = j\n        feats[f'{f}_stetson_k'] = k\n        \n        feats[f'{f}_max'] = np.max(flux)\n        feats[f'{f}_std'] = np.std(flux)\n        if f in ['g', 'r']:\n            current_max = np.max(flux)\n            if current_max > max_flux_global:\n                max_flux_global = current_max\n                peak_time = df_f.loc[df_f['Flux'].idxmax(), 'Time_Rel']\n\n        if f in ['g', 'r', 'i']:\n            bazin = fit_bazin(time, flux, flux_err)\n            for k_bazin, v_bazin in bazin.items():\n                feats[f'bazin_{f}_{k_bazin}'] = v_bazin\n            \n            if not np.isnan(bazin['tau_rise']) and bazin['tau_fall'] > 0:\n                feats[f'{f}_rise_fall_ratio'] = bazin['tau_rise'] / bazin['tau_fall']\n            else:\n                feats[f'{f}_rise_fall_ratio'] = np.nan\n        \n        filter_data[f] = (time, flux, flux_err)\n\n    if not np.isnan(peak_time):\n        if 'g_max' in feats:\n            feats['abs_mag_g'] = calculate_abs_mag(feats['g_max'], redshift)\n        elif 'r_max' in feats:\n            feats['abs_mag_g'] = calculate_abs_mag(feats['r_max'], redshift) # Fallback\n        else:\n            feats['abs_mag_g'] = np.nan\n\n        flux_at_peak = {}\n        flux_at_post = {} \n        dt_post = 20.0\n        \n        for f in ['g', 'r']:\n            if f in filter_data:\n                t, y_flux, y_err = filter_data[f]\n                flux_at_peak[f] = get_gp_prediction(t, y_flux, y_err, peak_time)\n                flux_at_post[f] = get_gp_prediction(t, y_flux, y_err, peak_time + dt_post)\n        \n        val_g0, val_r0 = flux_at_peak.get('g', np.nan), flux_at_peak.get('r', np.nan)\n        val_g1, val_r1 = flux_at_post.get('g', np.nan), flux_at_post.get('r', np.nan)\n        \n        if not np.any(np.isnan([val_g0, val_r0, val_g1, val_r1])) and val_g0>0 and val_r0>0:\n            color_0 = -2.5 * np.log10(val_g0 / val_r0)\n            color_1 = -2.5 * np.log10(val_g1 / val_r1)\n            \n            feats['color_slope_g_r'] = (color_1 - color_0) / dt_post\n            feats['color_g_r_peak'] = color_0\n        else:\n            feats['color_slope_g_r'] = np.nan\n            feats['color_g_r_peak'] = np.nan\n            \n    return feats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:38:09.008398Z","iopub.execute_input":"2025-12-24T11:38:09.008960Z","iopub.status.idle":"2025-12-24T11:38:09.022231Z","shell.execute_reply.started":"2025-12-24T11:38:09.008931Z","shell.execute_reply":"2025-12-24T11:38:09.021667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_features_parallel(log_df, data_path, n_jobs=-1):\n    print(\"Loading raw lightcurves...\")\n    all_chunks = []\n    unique_splits = log_df['split'].unique()\n    \n    print(log_df.columns)\n    z_col = log_df.columns[1]\n    if z_col not in log_df.columns:\n        print(\"Warning: Redshift column not found. Luminosity features will be NaN.\")\n        z_map = {oid: np.nan for oid in log_df['object_id']}\n    else:\n        z_map = dict(zip(log_df['object_id'], log_df[z_col]))\n\n    for split in unique_splits:\n        is_train = 'target' in log_df.columns\n        filename = 'train_full_lightcurves.csv' if is_train else 'test_full_lightcurves.csv'\n        path = os.path.join(data_path, split, filename)\n        if os.path.exists(path):\n            df_chunk = pd.read_csv(path)\n            valid_ids = set(log_df[log_df['split'] == split]['object_id'])\n            df_chunk = df_chunk[df_chunk['object_id'].isin(valid_ids)]\n            all_chunks.append(df_chunk)\n            \n    full_lc = pd.concat(all_chunks)\n    grouped = full_lc.groupby('object_id')\n    object_ids = list(grouped.groups.keys())\n    \n    print(f\"Extracting features for {len(object_ids)} objects...\")\n    \n    results = Parallel(n_jobs=n_jobs, backend='loky')(\n        delayed(process_single_object)(\n            obj_id, \n            grouped.get_group(obj_id), \n            z_map.get(obj_id, np.nan) \n        )\n        for obj_id in tqdm(object_ids)\n    )\n    \n    return pd.DataFrame(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:38:12.317981Z","iopub.execute_input":"2025-12-24T11:38:12.318628Z","iopub.status.idle":"2025-12-24T11:38:12.325557Z","shell.execute_reply.started":"2025-12-24T11:38:12.318574Z","shell.execute_reply":"2025-12-24T11:38:12.325006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- PROCESSING TRAIN DATA ---\")\ntrain_log = pd.read_csv(os.path.join(DATA_PATH, 'train_log.csv'))\n\ndf_train_features = extract_features_parallel(train_log, DATA_PATH, n_jobs=4)\n\ndf_train_final = train_log.merge(df_train_features, on='object_id', how='left')\nprint(f\"Train Data Shape: {df_train_final.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:38:16.675458Z","iopub.execute_input":"2025-12-24T11:38:16.675766Z","iopub.status.idle":"2025-12-24T11:43:05.603149Z","shell.execute_reply.started":"2025-12-24T11:38:16.675740Z","shell.execute_reply":"2025-12-24T11:43:05.602580Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:43:56.501190Z","iopub.execute_input":"2025-12-24T11:43:56.501802Z","iopub.status.idle":"2025-12-24T11:43:56.505556Z","shell.execute_reply.started":"2025-12-24T11:43:56.501776Z","shell.execute_reply":"2025-12-24T11:43:56.505017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ignore_cols = ['object_id', 'target', 'split', 'English Translation', 'SpecType']\nfeatures = [c for c in df_train_final.columns if c not in ignore_cols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:43:58.488243Z","iopub.execute_input":"2025-12-24T11:43:58.488994Z","iopub.status.idle":"2025-12-24T11:43:58.492188Z","shell.execute_reply.started":"2025-12-24T11:43:58.488966Z","shell.execute_reply":"2025-12-24T11:43:58.491700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df_train_final[features].copy()\n\nX = X.replace([np.inf, -np.inf], np.nan)\n\nX = X.fillna(0)\ny = df_train_final['target']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:43:59.724308Z","iopub.execute_input":"2025-12-24T11:43:59.724909Z","iopub.status.idle":"2025-12-24T11:43:59.734702Z","shell.execute_reply.started":"2025-12-24T11:43:59.724881Z","shell.execute_reply":"2025-12-24T11:43:59.734222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in X.select_dtypes(include=['object']).columns:\n    le = LabelEncoder()\n    X[col] = le.fit_transform(X[col].astype(str))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:44:01.235264Z","iopub.execute_input":"2025-12-24T11:44:01.235552Z","iopub.status.idle":"2025-12-24T11:44:01.239266Z","shell.execute_reply.started":"2025-12-24T11:44:01.235528Z","shell.execute_reply":"2025-12-24T11:44:01.238801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(len(X))\nmodels_ensemble = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:44:03.675374Z","iopub.execute_input":"2025-12-24T11:44:03.676156Z","iopub.status.idle":"2025-12-24T11:44:03.679742Z","shell.execute_reply.started":"2025-12-24T11:44:03.676117Z","shell.execute_reply":"2025-12-24T11:44:03.679289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from catboost import CatBoostClassifier ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:44:06.284426Z","iopub.execute_input":"2025-12-24T11:44:06.285066Z","iopub.status.idle":"2025-12-24T11:44:06.287858Z","shell.execute_reply.started":"2025-12-24T11:44:06.285040Z","shell.execute_reply":"2025-12-24T11:44:06.287225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_log = pd.read_csv(os.path.join(DATA_PATH, 'test_log.csv'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:44:07.961874Z","iopub.execute_input":"2025-12-24T11:44:07.962519Z","iopub.status.idle":"2025-12-24T11:44:08.000988Z","shell.execute_reply.started":"2025-12-24T11:44:07.962496Z","shell.execute_reply":"2025-12-24T11:44:08.000569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test_features = extract_features_parallel(test_log, DATA_PATH, n_jobs=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:44:09.380249Z","iopub.execute_input":"2025-12-24T11:44:09.380961Z","iopub.status.idle":"2025-12-24T11:55:17.885010Z","shell.execute_reply.started":"2025-12-24T11:44:09.380930Z","shell.execute_reply":"2025-12-24T11:55:17.884360Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test_final = test_log.merge(df_test_features, on='object_id', how='left')\nprint(f\"Test Data Shape: {df_test_final.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:55:19.994492Z","iopub.execute_input":"2025-12-24T11:55:19.995067Z","iopub.status.idle":"2025-12-24T11:55:20.009261Z","shell.execute_reply.started":"2025-12-24T11:55:19.995037Z","shell.execute_reply":"2025-12-24T11:55:20.008668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n    \n    print(f\"Fold {fold+1}: Running SMOTE...\")\n    try:\n        smote = SMOTE(sampling_strategy=0.2, random_state=42)\n        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n    except ValueError:\n        print(\"SMOTE failed, using raw data.\")\n        X_train_res, y_train_res = X_train, y_train\n\n    clf_lgb = lgb.LGBMClassifier(\n        objective='binary', boosting_type='gbdt',\n        verbose=-1, random_state=42, n_jobs=-1,\n        learning_rate=0.00820959390156126,\n        n_estimators=1456,\n        num_leaves=21,\n        max_depth=7,\n        min_child_samples=18,\n        subsample=0.8056429645734329,\n        colsample_bytree=0.7037094223073973,\n        reg_alpha=1.2090883197470177,\n        reg_lambda=3.5948946953880816e-08\n    )\n    \n    clf_xgb = XGBClassifier(\n        objective='binary:logistic', eval_metric='logloss',\n        random_state=42, use_label_encoder=False,\n        tree_method='hist',\n        device='cuda' if USE_GPU else 'cpu',\n        learning_rate=0.0205217807496452,\n        n_estimators=1185,\n        max_depth=7,\n        min_child_weight=7,\n        subsample=0.7753093208264389,\n        colsample_bytree=0.8265862295956217,\n        gamma=1.0058203352400668,\n        reg_alpha=4.053334043323454e-06,\n        reg_lambda=0.09447983849476049\n    )\n    \n    clf_cat = CatBoostClassifier(\n        loss_function='Logloss', eval_metric='F1',\n        verbose=0, random_seed=42, allow_writing_files=False,\n        task_type='CPU'\n        learning_rate=0.04297575732378107,\n        iterations=735,\n        depth=7,\n        l2_leaf_reg=5.44676657488797,\n        border_count=182,\n        random_strength=2.5066619218798087,\n        bagging_temperature=0.9978326033997912\n    )\n    \n    eclf = VotingClassifier(\n        estimators=[('lgb', clf_lgb), ('xgb', clf_xgb), ('cat', clf_cat)],\n        voting='soft',\n        weights=[1, 1, 1] \n    )\n    \n    eclf.fit(X_train_res, y_train_res)\n    \n    val_probs = eclf.predict_proba(X_val)[:, 1]\n    oof_preds[val_idx] = val_probs\n    \n    best_f1, best_th = 0, 0.5\n    for th in np.linspace(0.1, 0.9, 100):\n        score = f1_score(y_val, (val_probs > th).astype(int))\n        if score > best_f1: best_f1, best_th = score, th\n            \n    print(f\"Fold {fold+1} Trifecta F1: {best_f1:.4f} (Th: {best_th:.2f})\")\n    models_ensemble.append(eclf)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"thresholds = np.linspace(0.01, 0.99, 200)\nf1_list = [f1_score(y, (oof_preds > t).astype(int)) for t in thresholds]\nglobal_best_thresh = thresholds[np.argmax(f1_list)]\nprint(f\"\\nGlobal Best F1: {np.max(f1_list):.4f} at Threshold: {global_best_thresh:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport pandas as pd\n\nignore_cols = ['object_id', 'target', 'split', 'English Translation', 'SpecType']\nfeatures = [c for c in df_train_final.columns if c not in ignore_cols]\n\nX = df_train_final[features].copy().replace([np.inf, -np.inf], np.nan)\ny = df_train_final['target']\n\nfor col in X.select_dtypes(include=['object']).columns:\n    le = LabelEncoder()\n    X[col] = le.fit_transform(X[col].astype(str))\n\nneg = (y==0).sum()\npos = (y==1).sum()\nscale_pos_weight = neg / pos\n\ndef objective(trial):\n    param = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'n_jobs': -1,\n        'random_state': 42,\n        'scale_pos_weight': scale_pos_weight,\n        \n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n        'n_estimators': trial.suggest_int('n_estimators', 400, 1500),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n        'max_depth': trial.suggest_int('max_depth', 5, 15),\n        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n        'subsample': trial.suggest_float('subsample', 0.5, 0.95),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.95),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n    }\n\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    f1_scores = []\n    \n    for train_idx, val_idx in skf.split(X, y):\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n        \n        model = lgb.LGBMClassifier(**param)\n        model.fit(X_train, y_train)\n        \n        # Tìm ngưỡng tối ưu nội bộ cho fold này\n        preds_proba = model.predict_proba(X_val)[:, 1]\n        best_f1 = 0\n        for th in np.linspace(0.1, 0.9, 20):\n            score = f1_score(y_val, (preds_proba > th).astype(int))\n            if score > best_f1: best_f1 = score\n        \n        f1_scores.append(best_f1)\n        \n    return np.mean(f1_scores)\n\nprint(\"Starting Optuna Tuning for LightGBM...\")\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\n\nprint(f\"Best F1: {study.best_value:.4f}\")\nprint(\"Best Params:\")\nfor key, value in study.best_params.items():\n    print(f\"    '{key}': {value},\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nimport numpy as np\nimport torch\n\nif 'scale_pos_weight' not in globals():\n    neg = (y==0).sum()\n    pos = (y==1).sum()\n    scale_pos_weight = neg / pos\n\nUSE_GPU = torch.cuda.is_available()\n\ndef objective_xgb(trial):\n    param = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        \n        'tree_method': 'hist',   \n        'device': 'cuda' if USE_GPU else 'cpu',\n        \n        'random_state': 42,\n        'use_label_encoder': False,\n        'scale_pos_weight': scale_pos_weight,\n        \n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n        'n_estimators': trial.suggest_int('n_estimators', 500, 1500),\n        'max_depth': trial.suggest_int('max_depth', 4, 12),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'subsample': trial.suggest_float('subsample', 0.5, 0.95),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.95),\n        'gamma': trial.suggest_float('gamma', 0, 5),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n    }\n\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    f1_scores = []\n    \n    for train_idx, val_idx in skf.split(X, y):\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n        \n        model = XGBClassifier(**param)\n        model.fit(X_train, y_train)\n        \n        preds_proba = model.predict_proba(X_val)[:, 1]\n        best_f1 = 0\n        for th in np.linspace(0.1, 0.9, 20):\n            score = f1_score(y_val, (preds_proba > th).astype(int))\n            if score > best_f1: best_f1 = score\n        f1_scores.append(best_f1)\n        \n    return np.mean(f1_scores)\n\nprint(\"Starting Optuna for XGBoost...\")\nstudy_xgb = optuna.create_study(direction='maximize')\nstudy_xgb.optimize(objective_xgb, n_trials=30)\nprint(\"Best XGB Params:\", study_xgb.best_params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nimport numpy as np\n\ndef objective_cat(trial):\n    param = {\n        'loss_function': 'Logloss',\n        'eval_metric': 'F1',\n        'task_type': 'CPU',  \n        'verbose': 0,\n        'random_seed': 42,\n        'allow_writing_files': False,\n        'auto_class_weights': 'Balanced',\n        \n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n        'iterations': trial.suggest_int('iterations', 500, 1500),\n        'depth': trial.suggest_int('depth', 4, 10),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n        'border_count': trial.suggest_int('border_count', 32, 255),\n        'random_strength': trial.suggest_float('random_strength', 0, 10),\n        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1)\n    }\n\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    f1_scores = []\n    \n    for train_idx, val_idx in skf.split(X, y):\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n        \n        model = CatBoostClassifier(**param)\n        model.fit(X_train, y_train)\n        \n        preds_proba = model.predict_proba(X_val)[:, 1]\n        best_f1 = 0\n        for th in np.linspace(0.1, 0.9, 20):\n            score = f1_score(y_val, (preds_proba > th).astype(int))\n            if score > best_f1: best_f1 = score\n        f1_scores.append(best_f1)\n        \n    return np.mean(f1_scores)\n\nprint(\"Starting Optuna for CatBoost (CPU Mode)...\")\nstudy_cat = optuna.create_study(direction='maximize')\nstudy_cat.optimize(objective_cat, n_trials=30)\nprint(\"Best CatBoost Params:\", study_cat.best_params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test = df_test_final[features].copy()\nX_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n\nfor col in X_test.select_dtypes(include=['object']).columns:\n    X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(0)\n\ntest_probs = np.zeros(len(X_test))\nfor model in models_ensemble:\n    test_probs += model.predict_proba(X_test)[:, 1] / len(models_ensemble)\n\npredictions = (test_probs > global_best_thresh).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T12:00:15.606073Z","iopub.execute_input":"2025-12-24T12:00:15.606800Z","iopub.status.idle":"2025-12-24T12:00:15.623670Z","shell.execute_reply.started":"2025-12-24T12:00:15.606764Z","shell.execute_reply":"2025-12-24T12:00:15.622825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'object_id': df_test_final['object_id'],\n    'prediction': predictions\n})\n\nsubmission.to_csv('submission_2.csv', index=False)\nprint(\"\\nSuccess! Saved submission.csv with SMOTE & Ensemble.\")\nprint(submission.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in X.select_dtypes(include=['object']).columns:\n    le = LabelEncoder()\n    full_col = pd.concat([X[col].astype(str), X_test[col].astype(str)])\n    le.fit(full_col)\n    X[col] = le.transform(X[col].astype(str))\n    X_test[col] = le.transform(X_test[col].astype(str))\n\nneg = (y==0).sum()\npos = (y==1).sum()\nsqrt_weight = np.sqrt(neg / pos) \nprint(f\"Original Weight Ratio: {neg/pos:.2f} -> Adjusted (Sqrt): {sqrt_weight:.2f}\")\n\nUSE_GPU = torch.cuda.is_available()\n\nclf_lgb = lgb.LGBMClassifier(\n    objective='binary', boosting_type='dart',\n    learning_rate=0.05, n_estimators=1000,\n    num_leaves=31, max_depth=-1,\n    scale_pos_weight=sqrt_weight, \n    colsample_bytree=0.7, subsample=0.7,\n    n_jobs=-1, random_state=42, verbose=-1\n)\n\nclf_xgb = XGBClassifier(\n    objective='binary:logistic', eval_metric='logloss',\n    learning_rate=0.03, n_estimators=1000, max_depth=6,\n    scale_pos_weight=sqrt_weight,\n    colsample_bytree=0.7, subsample=0.7,\n    tree_method='hist', device='cuda' if USE_GPU else 'cpu',\n    use_label_encoder=False, random_state=42\n)\n\nclf_cat = CatBoostClassifier(\n    loss_function='Logloss', eval_metric='F1',\n    learning_rate=0.03, iterations=1000, depth=6,\n    auto_class_weights='SqrtBalanced',\n    task_type='CPU',\n    verbose=0, random_seed=42, allow_writing_files=False\n)\n\neclf = VotingClassifier(\n    estimators=[('lgb', clf_lgb), ('xgb', clf_xgb), ('cat', clf_cat)],\n    voting='soft', weights=[1, 1, 1]\n)\n\nprint(\"Training on Full Dataset...\")\neclf.fit(X, y)\n\nprint(\"Predicting Test Data...\")\ntest_probs = eclf.predict_proba(X_test)[:, 1]\n\ntrain_tde_rate = y.mean()\nprint(f\"TDE Rate in Train: {train_tde_rate:.2%}\")\n\ntarget_percentile = 100 * (1 - train_tde_rate * 1.1) \ndynamic_threshold = np.percentile(test_probs, target_percentile)\n\nprint(f\"Dynamic Threshold (Top {train_tde_rate*1.1:.1%} predictions): {dynamic_threshold:.4f}\")\n\npredictions = (test_probs > dynamic_threshold).astype(int)\n\nn_tde_pred = predictions.sum()\nprint(f\"Predicted {n_tde_pred} TDEs out of {len(predictions)} objects ({n_tde_pred/len(predictions):.2%})\")\n\nsubmission = pd.DataFrame({\n    'object_id': df_test_final['object_id'],\n    'prediction': predictions\n})\n\nsubmission.to_csv('submission_final_physics.csv', index=False)\nprint(\"\\nSuccess! Saved submission_final.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T12:00:47.825004Z","iopub.execute_input":"2025-12-24T12:00:47.825776Z","iopub.status.idle":"2025-12-24T12:01:05.805428Z","shell.execute_reply.started":"2025-12-24T12:00:47.825748Z","shell.execute_reply":"2025-12-24T12:01:05.804855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T12:02:33.229127Z","iopub.execute_input":"2025-12-24T12:02:33.229734Z","iopub.status.idle":"2025-12-24T12:50:41.991254Z","shell.execute_reply.started":"2025-12-24T12:02:33.229704Z","shell.execute_reply":"2025-12-24T12:50:41.990639Z"}},"outputs":[],"execution_count":null}]}